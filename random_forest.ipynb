{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1011e757",
   "metadata": {},
   "source": [
    "## Scikit-Learn Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e047d1e",
   "metadata": {},
   "source": [
    "**Summary:** \n",
    "This notebook was used to run the Sciki-learn random forest model experiments as an addition to the experiments run using Edge Impulse. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install visualkeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edba98f",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7695839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "from scipy.io.wavfile import WavFileWarning\n",
    "from sklearn.metrics import accuracy_score\n",
    "import visualkeras\n",
    "\n",
    "# dependencies for compute_multiclass_metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from everywhereml.sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796caeb",
   "metadata": {},
   "source": [
    "## Path Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d533b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "n_mel = 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8bfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f'/home/bukowskin/CSC_7901_ML_Capstone/data/data_split_random_state_{random_state}/number_mel_filters_{n_mel}'\n",
    "\n",
    "mfe_data_path = f'{data_path}/mfe_data_split'\n",
    "mfcc_data_path = f'{data_path}/mfcc_data_split'\n",
    "\n",
    "save_model_path = f'/home/bukowskin/CSC_7901_ML_Capstone/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803aa78",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ebe725",
   "metadata": {},
   "source": [
    "### Load MFE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e06b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{mfe_data_path}/train_data.pkl', 'rb') as f:\n",
    "    mfe_train = pickle.load(f)\n",
    "    \n",
    "with open(f'{mfe_data_path}/test_data.pkl', 'rb') as f:\n",
    "    mfe_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "train_mfe_feat = mfe_train['features']\n",
    "y_train = mfe_train['labels']\n",
    "\n",
    "# test \n",
    "test_mfe_feat = mfe_test['features']\n",
    "y_test = mfe_test['labels']\n",
    "\n",
    "# height and width\n",
    "mfe_height = mfe_train['mfe_height']\n",
    "mfe_width = mfe_train['mfe_width']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e685ac",
   "metadata": {},
   "source": [
    "### Load MFCC  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82059915",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{mfcc_data_path}/train_data.pkl', 'rb') as f:\n",
    "    mfcc_train = pickle.load(f)\n",
    "    \n",
    "with open(f'{mfcc_data_path}/test_data.pkl', 'rb') as f:\n",
    "    mfcc_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "train_mfcc_feat = mfcc_train['features']\n",
    "\n",
    "# test \n",
    "test_mfcc_feat = mfcc_test['features']\n",
    "\n",
    "# height and width\n",
    "mfcc_height = mfcc_train['mfe_height']\n",
    "mfcc_width = mfcc_train['mfe_width']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9adc54",
   "metadata": {},
   "source": [
    "### Prepare Data\n",
    "\n",
    "Taking the computed height and width and reshaping features for model input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe4ce7",
   "metadata": {},
   "source": [
    "### MFE Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_mfe_feat = np.array(train_mfe_feat)\n",
    "print(f'Training MFE Features Shape: {train_mfe_feat.shape}')\n",
    "\n",
    "# test\n",
    "test_mfe_feat = np.array(test_mfe_feat)\n",
    "print(f'Testing MFE Features Shape: {test_mfe_feat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46b367",
   "metadata": {},
   "source": [
    "### MFCC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1592ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_mfcc_feat = np.array(train_mfcc_feat)\n",
    "print(f'Training MFCC Features Shape: {train_mfcc_feat.shape}')\n",
    "\n",
    "# test\n",
    "test_mfcc_feat = np.array(test_mfcc_feat)\n",
    "print(f'MFCC Features Shape: {test_mfcc_feat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba28b63e",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a715984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# encoding labels to numerical representations\n",
    "train_label_encoder = LabelEncoder()\n",
    "y_train = train_label_encoder.fit_transform(y_train)\n",
    "label_mapping = dict(zip(range(len(train_label_encoder.classes_)),train_label_encoder.classes_,))\n",
    "\n",
    "\n",
    "# test\n",
    "# encoding labels to numerical representations\n",
    "test_label_encoder = LabelEncoder()\n",
    "y_test = test_label_encoder.fit_transform(y_test)\n",
    "label_mapping = dict(zip(range(len(test_label_encoder.classes_)),test_label_encoder.classes_))\n",
    "\n",
    "\n",
    "print(f'Label mapping: {label_mapping}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95910e01",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366f964",
   "metadata": {},
   "source": [
    "## MFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a712a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and validation sets like in edge impulse \n",
    "X_train, X_val, y_train, y_val = train_test_split(train_mfe_feat, y_train, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460eb420",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 3\n",
    "\n",
    "accuracies = []\n",
    "auc_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "n_trees = 100\n",
    "for trial in range(NUM_TRIALS):\n",
    "    print(f\"Trial {trial + 1}/{NUM_TRIALS}\")\n",
    "\n",
    "    # train \n",
    "    rf_mfe = RandomForestClassifier(n_estimators = n_trees)\n",
    "    rf_mfe.fit(X_train, y_train)\n",
    "\n",
    "    # predict\n",
    "    y_pred_probs = rf_mfe.predict_proba(test_mfe_feat)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    # metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='weighted'\n",
    "    )\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    # binarize true labels for AUC \n",
    "    y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    auc_score = roc_auc_score(y_test_bin, y_pred_probs, average='micro')\n",
    "    auc_scores.append(auc_score)\n",
    "\n",
    "# calc avg and std\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "\n",
    "avg_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "\n",
    "avg_recall = np.mean(recalls)\n",
    "std_recall = np.std(recalls)\n",
    "\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "std_f1_score = np.std(f1_scores)\n",
    "\n",
    "avg_auc = np.mean(auc_scores)\n",
    "std_auc = np.std(auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac2c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nResults (averaged over 3 trials):\")\n",
    "print(f'Number Estimators: {n_trees}')\n",
    "print(f'Number of filters: {n_mel}')\n",
    "print(f\"Accuracy: {avg_accuracy*100:.2f} ± {std_accuracy*100:.2f}\")\n",
    "print(f\"Precision: {avg_precision:.2f} ± {std_precision:.2f}\")\n",
    "print(f\"Recall: {avg_recall:.2f} ± {std_recall:.2f}\")\n",
    "print(f\"F1-Score: {avg_f1_score:.2f} ± {std_f1_score:.2f}\")\n",
    "print(f\"AUC: {avg_auc:.2f} ± {std_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a35e2",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c9e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/home/bukowskin/CSC_7901_ML_Capstone/models/RandomForestClassifier_{n_mel}_{n_trees}_mfe.h', 'w') as file:\n",
    "    file.write(rf_mfe.to_arduino(instance_name='RandomForestClassifier'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c5fa88",
   "metadata": {},
   "source": [
    "## MFCC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb53a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and validation sets like in edge impulse \n",
    "X_train, X_val, y_train, y_val = train_test_split(train_mfcc_feat, y_train, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69741b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 3\n",
    "\n",
    "accuracies = []\n",
    "auc_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "n_trees = 50\n",
    "\n",
    "for trial in range(NUM_TRIALS):\n",
    "    \n",
    "    print(f\"Trial {trial + 1}/{NUM_TRIALS}\")\n",
    "    \n",
    "    # train \n",
    "    rf_mfcc = RandomForestClassifier(n_estimators = n_trees)\n",
    "    rf_mfcc.fit(X_train, y_train)\n",
    "\n",
    "    # predict \n",
    "    y_pred_probs = rf_mfcc.predict_proba(test_mfcc_feat)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    # metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='weighted'\n",
    "    )\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    # binarize true labels for AUC \n",
    "    y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    auc_score = roc_auc_score(y_test_bin, y_pred_probs, average='micro')\n",
    "    auc_scores.append(auc_score)\n",
    "\n",
    "# calc avg and std\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "\n",
    "avg_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "\n",
    "avg_recall = np.mean(recalls)\n",
    "std_recall = np.std(recalls)\n",
    "\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "std_f1_score = np.std(f1_scores)\n",
    "\n",
    "avg_auc = np.mean(auc_scores)\n",
    "std_auc = np.std(auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc91818",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nResults (averaged over 3 trials):\")\n",
    "print(f'Number Estimators: {n_trees}')\n",
    "print(f'Number of filters: {n_mel}')\n",
    "print(f\"Accuracy: {avg_accuracy*100:.2f} ± {std_accuracy*100:.2f}\")\n",
    "print(f\"Precision: {avg_precision:.2f} ± {std_precision:.2f}\")\n",
    "print(f\"Recall: {avg_recall:.2f} ± {std_recall:.2f}\")\n",
    "print(f\"F1-Score: {avg_f1_score:.2f} ± {std_f1_score:.2f}\")\n",
    "print(f\"AUC: {avg_auc:.2f} ± {std_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d4ca4",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "Using everywhereml library to generate corresponding C code for Random Forest [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de478d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/home/bukowskin/CSC_7901_ML_Capstone/models/RandomForestClassifier_{n_mel}_{n_trees}_mfcc.h', 'w') as file:\n",
    "    file.write(rf_mfcc.to_arduino(instance_name='RandomForestClassifier'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f9e0e8",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a92f64",
   "metadata": {},
   "source": [
    "[1] S. Salerno, W. Flinn and V. , \"everywhereml,\" 2021. [Online]. Available: https://github.com/eloquentarduino/everywhereml."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
