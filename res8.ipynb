{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1011e757",
   "metadata": {},
   "source": [
    "## Residual Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e047d1e",
   "metadata": {},
   "source": [
    "**Summary:** \n",
    "\n",
    "The purpose of this notebook is to  experiment with the ResNet architecture for specifically key-word spotting. The model applied is called the `res8-narrow`. This is model comes from a reduced base model, `resnet15`, where the authors proposed to reduce the number of residual blocks from 6 to 3 to allow a smaller footprint model that has a reduced network depth. Additionally, the authors compacted model includes a 4x3 average-pooling layer after the first convolution layer to reduce the size of the time and frequency dimensions by a factor of four and three. Lastly they decided to omit dilated convolutions sinces the average-pooling layer had reduced the input dimension.\n",
    "Both models come from \"Deep Residual Learning for Small-Footprint Keyword Spotting\" by Raphael Tang and Jimmy Lin [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7562eab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: visualkeras in /home/bukowskin/.local/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/bukowskin/.local/lib/python3.10/site-packages (from visualkeras) (9.0.1)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from visualkeras) (1.24.4)\n",
      "Requirement already satisfied: aggdraw>=1.3.11 in /home/bukowskin/.local/lib/python3.10/site-packages (from visualkeras) (1.3.19)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install visualkeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edba98f",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7695839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 10:15:24.113010: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-17 10:15:24.859149: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-17 10:15:24.861054: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-17 10:15:25.049869: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-17 10:15:25.361739: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/bukowskin/.local/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import callbacks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "from scipy.io.wavfile import WavFileWarning\n",
    "from sklearn.metrics import accuracy_score\n",
    "import visualkeras\n",
    "from visualkeras import SpacingDummyLayer\n",
    "\n",
    "warnings.simplefilter(\"ignore\", WavFileWarning)\n",
    "# conversion causes lots of pink warning errors\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # '0' = all logs, '1' = info, '2' = warnings, '3' = errors only\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# dependencies for compute_multiclass_metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796caeb",
   "metadata": {},
   "source": [
    "## Path Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d533b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "n_mel = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8bfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f'/home/bukowskin/CSC_7901_ML_Capstone/data/data_split_random_state_{random_state}/number_mel_filters_{n_mel}'\n",
    "\n",
    "mfe_data_path = f'{data_path}/mfe_data_split'\n",
    "mfcc_data_path = f'{data_path}/mfcc_data_split'\n",
    "\n",
    "save_model_path = f'/home/bukowskin/CSC_7901_ML_Capstone/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf75e917",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6c85b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuarcy_after_quan(model_path, test_data, y_true, feat_type, trial):\n",
    "    \"\"\"\n",
    "    Function used to check model accuracy after TensorFlow Lite conversion with quanitization applied. Stated\n",
    "    there can be a possible accuracy change. \n",
    "        Args:\n",
    "            model_path(str): path to saved model\n",
    "            test_data (np.array): testing data\n",
    "            y_true (np.array): true labels for testing data\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # load TensorFlow Lite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()  \n",
    "\n",
    "    # get input and output tensor details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # initialize an empty list for storing predictions\n",
    "    y_pred_probs = []\n",
    "\n",
    "    # setting batch size for prediction \n",
    "    batch_size = 64\n",
    "    num_batches = len(test_data) // batch_size\n",
    "\n",
    "    # process sample by sample \n",
    "    for i in range(test_data.shape[0]):\n",
    "        sample_data = test_data[i:i+1]  # select one sample, shape is (1, height, width, channels)\n",
    "\n",
    "        # making sure data is in the correct format (float32)\n",
    "        sample_data = sample_data.astype(np.float32)\n",
    "\n",
    "        # setting input tensor for the current sample\n",
    "        interpreter.set_tensor(input_details[0]['index'], sample_data)\n",
    "\n",
    "        # running inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # getting output tensor (the predictions)\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        # appending predictions to the list\n",
    "        y_pred_probs.append(output_data)\n",
    "\n",
    "    # combining predictions from all samples into a single NumPy array\n",
    "    y_pred_probs = np.vstack(y_pred_probs)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1) \n",
    "\n",
    "    # accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    if trial:\n",
    "        return accuracy\n",
    "    \n",
    "    if feat_type == 'mfe':\n",
    "        print(f'MFE Feature Accuracy: {accuracy * 100:.2f}%')\n",
    "    elif feat_type == 'mfcc':\n",
    "        print(f'MFCC Feature Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dada38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multiclass_metrics(y_true, y_pred_probs, label_mapping):\n",
    "    \"\"\"\n",
    "    Function to compute and plot One-vs-Rest ROC curves and calculate weighted averages for metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Array of true labels (integer-encoded).\n",
    "        y_pred_probs: Array of predicted probabilities for each class.\n",
    "        label_mapping: Dictionary mapping class indices to human-readable names.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    n_classes = len(label_mapping) \n",
    "    y_true_bin = label_binarize(y_true, classes=list(label_mapping.keys())) \n",
    "\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # plotting ROC\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(\n",
    "            fpr[i], tpr[i], lw=2,\n",
    "            label=f'Class {label_mapping[i]} (AUC = {roc_auc[i]:.2f})'\n",
    "        )\n",
    "    \n",
    "    # plotting random guess line \n",
    "    plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multiclass ROC Curve (One-vs-Rest)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # weighted avg metrics\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted'\n",
    "    )\n",
    "    print(f\"Weighted Average Precision: {precision:.2f}\")\n",
    "    print(f\"Weighted Average Recall: {recall:.2f}\")\n",
    "    print(f\"Weighted Average F1-Score: {f1_score:.2f}\")\n",
    "\n",
    "    # micro-avg AUC\n",
    "    micro_auc = roc_auc_score(y_true_bin, y_pred_probs, average='micro')\n",
    "    print(f\"Micro-Average AUC: {micro_auc:.2f}\")\n",
    "\n",
    "    # report\n",
    "    class_report = classification_report(\n",
    "        y_true, y_pred, target_names=[f\"Class {label_mapping[i]}\" for i in range(n_classes)]\n",
    "    )\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803aa78",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ebe725",
   "metadata": {},
   "source": [
    "### Load MFE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2e06b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{mfe_data_path}/train_data.pkl', 'rb') as f:\n",
    "    mfe_train = pickle.load(f)\n",
    "    \n",
    "with open(f'{mfe_data_path}/test_data.pkl', 'rb') as f:\n",
    "    mfe_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3995b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "train_mfe_feat = mfe_train['features']\n",
    "y_train = mfe_train['labels']\n",
    "\n",
    "# test \n",
    "test_mfe_feat = mfe_test['features']\n",
    "y_test = mfe_test['labels']\n",
    "\n",
    "# height and width\n",
    "mfe_height = mfe_train['mfe_height']\n",
    "mfe_width = mfe_train['mfe_width']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e685ac",
   "metadata": {},
   "source": [
    "### Load MFCC  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82059915",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{mfcc_data_path}/train_data.pkl', 'rb') as f:\n",
    "    mfcc_train = pickle.load(f)\n",
    "    \n",
    "with open(f'{mfcc_data_path}/test_data.pkl', 'rb') as f:\n",
    "    mfcc_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "925f0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "train_mfcc_feat = mfcc_train['features']\n",
    "\n",
    "# test \n",
    "test_mfcc_feat = mfcc_test['features']\n",
    "\n",
    "# height and width\n",
    "mfcc_height = mfcc_train['mfe_height']\n",
    "mfcc_width = mfcc_train['mfe_width']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9adc54",
   "metadata": {},
   "source": [
    "### Prepare Data\n",
    "\n",
    "Taking the computed height and width and reshaping features for model input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe4ce7",
   "metadata": {},
   "source": [
    "### MFE Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "047f514d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MFE Features Shape: (1055, 40, 50, 1)\n",
      "Testing MFE Features Shape: (264, 40, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_mfe_feat = np.array(train_mfe_feat)\n",
    "train_mfe_feat= train_mfe_feat.reshape(-1, mfe_height, mfe_width, 1)\n",
    "print(f'Training MFE Features Shape: {train_mfe_feat.shape}')\n",
    "\n",
    "# test\n",
    "test_mfe_feat = np.array(test_mfe_feat)\n",
    "test_mfe_feat= test_mfe_feat.reshape(-1, mfe_height, mfe_width, 1)\n",
    "print(f'Testing MFE Features Shape: {test_mfe_feat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46b367",
   "metadata": {},
   "source": [
    "### MFCC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1592ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MFCC Features Shape: (1055, 13, 50, 1)\n",
      "MFCC Features Shape: (264, 13, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_mfcc_feat = np.array(train_mfcc_feat)\n",
    "train_mfcc_feat= train_mfcc_feat.reshape(-1,mfcc_height, mfcc_width, 1)\n",
    "print(f'Training MFCC Features Shape: {train_mfcc_feat.shape}')\n",
    "\n",
    "# test\n",
    "test_mfcc_feat = np.array(test_mfcc_feat)\n",
    "test_mfcc_feat= test_mfcc_feat.reshape(-1, mfcc_height, mfcc_width, 1)\n",
    "print(f'MFCC Features Shape: {test_mfcc_feat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba28b63e",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a715984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {0: 'blue', 1: 'green', 2: 'one', 3: 'red', 4: 'three', 5: 'two'}\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "# encoding labels to numerical representations\n",
    "train_label_encoder = LabelEncoder()\n",
    "y_train = train_label_encoder.fit_transform(y_train)\n",
    "label_mapping = dict(zip(range(len(train_label_encoder.classes_)),train_label_encoder.classes_,))\n",
    "\n",
    "\n",
    "# test\n",
    "# encoding labels to numerical representations\n",
    "test_label_encoder = LabelEncoder()\n",
    "y_test = test_label_encoder.fit_transform(y_test)\n",
    "label_mapping = dict(zip(range(len(test_label_encoder.classes_)),test_label_encoder.classes_))\n",
    "\n",
    "\n",
    "print(f'Label mapping: {label_mapping}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95910e01",
   "metadata": {},
   "source": [
    "## ResNet8-Narrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b803037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters):\n",
    "    \"\"\"\n",
    "    Function for implementing standard ResNet architecture, residual block. This includes a 2 3x3 convolutional with \n",
    "    batch normalization and ReLu activation\n",
    "    Additionally, unlike the resnet15 (which the model deviates from), this one doesn't include a dimension adjustment.\n",
    "        Args:\n",
    "            filters(int): number of filters used for Conv2D \n",
    "        Returns:\n",
    "            tensor: processed data after Con2d, Relu and BatchNorm \n",
    "    \"\"\"\n",
    "\n",
    "    # save input for skip connection\n",
    "    shortcut = x\n",
    "\n",
    "    # 1st convolution layer\n",
    "    y = layers.Conv2D(filters, (3, 3), padding='same', use_bias=False)(x)\n",
    "    y = layers.ReLU()(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    \n",
    "\n",
    "    # 2nd convolution layer\n",
    "    y = layers.Conv2D(filters, (3, 3), padding='same', use_bias=False)(y)\n",
    "    y = layers.ReLU()(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "\n",
    "    # adding skip connection\n",
    "    y = layers.add([shortcut, y])\n",
    "\n",
    "    return y\n",
    "    \n",
    "\n",
    "def build_resnet_8(input_shape, num_classes,feat_maps):\n",
    "    \"\"\"\n",
    "    Function builds the ResNet model as described in the paper.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # initial convolution layer\n",
    "    x = layers.Conv2D(feat_maps, (3, 3), padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 4x3 average pooling \n",
    "    x = layers.AveragePooling2D(pool_size=(4, 3))(x)\n",
    "\n",
    "    # 3 residual blocks \n",
    "    for _ in range(3):\n",
    "        x = residual_block(x, feat_maps)\n",
    "\n",
    "    # global average pooling\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # fully connected softmax layer\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # initialize model\n",
    "    model = models.Model(inputs, outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6d270",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366f964",
   "metadata": {},
   "source": [
    "#### MFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e22fe4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 02:11:20.248778: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-12-15 02:11:20.248838: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: dh-node13\n",
      "2024-12-15 02:11:20.248851: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: dh-node13\n",
      "2024-12-15 02:11:20.248943: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got \"1\"\n",
      "2024-12-15 02:11:20.248976: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 555.42.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 40, 50, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 40, 50, 32)           288       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 40, 50, 32)           128       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 40, 50, 32)           0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d (Average  (None, 10, 16, 32)           0         ['re_lu[0][0]']               \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 10, 16, 32)           9216      ['average_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 10, 16, 32)           0         ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 10, 16, 32)           128       ['re_lu_1[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 10, 16, 32)           9216      ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 10, 16, 32)           0         ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 10, 16, 32)           128       ['re_lu_2[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 10, 16, 32)           0         ['average_pooling2d[0][0]',   \n",
      "                                                                     'batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 10, 16, 32)           9216      ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 10, 16, 32)           0         ['conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 10, 16, 32)           128       ['re_lu_3[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 10, 16, 32)           9216      ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 10, 16, 32)           0         ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 10, 16, 32)           128       ['re_lu_4[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 10, 16, 32)           0         ['add[0][0]',                 \n",
      "                                                                     'batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 10, 16, 32)           9216      ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 10, 16, 32)           0         ['conv2d_5[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 10, 16, 32)           128       ['re_lu_5[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 10, 16, 32)           9216      ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)              (None, 10, 16, 32)           0         ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 10, 16, 32)           128       ['re_lu_6[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 10, 16, 32)           0         ['add_1[0][0]',               \n",
      "                                                                     'batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling2d (  (None, 32)                   0         ['add_2[0][0]']               \n",
      " GlobalAveragePooling2D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 6)                    198       ['global_average_pooling2d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 56678 (221.40 KB)\n",
      "Trainable params: 56230 (219.65 KB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.1,\n",
    "    momentum=0.9, \n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# learning rate scheduler\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "input_shape_mfe = (mfe_height, mfe_width, 1) # (height, width, channels)\n",
    "\n",
    "# build the model\n",
    "model_mfe = build_resnet_8(input_shape_mfe, num_classes=6, feat_maps=32) # red, green, blue, one, two, three\n",
    "\n",
    "# model summary\n",
    "model_mfe.summary()\n",
    "\n",
    "# Compile the model\n",
    "model_mfe.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed72171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/26\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 2.0000 - accuracy: 0.2114 - lr: 0.1000\n",
      "Epoch 2/26\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 1.7993 - accuracy: 0.2303 - lr: 0.1000\n",
      "Epoch 3/26\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 1.6856 - accuracy: 0.2995 - lr: 0.1000\n",
      "Epoch 4/26\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 1.5904 - accuracy: 0.3460 - lr: 0.1000\n",
      "Epoch 5/26\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 1.5171 - accuracy: 0.3791 - lr: 0.1000\n",
      "Epoch 6/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 1.4301 - accuracy: 0.4085 - lr: 0.1000\n",
      "Epoch 7/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 1.3030 - accuracy: 0.4739 - lr: 0.1000\n",
      "Epoch 8/26\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 1.3007 - accuracy: 0.4967 - lr: 0.1000\n",
      "Epoch 9/26\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 1.1593 - accuracy: 0.5318 - lr: 0.1000\n",
      "Epoch 10/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 1.1487 - accuracy: 0.5374 - lr: 0.1000\n",
      "Epoch 11/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 1.0513 - accuracy: 0.5697 - lr: 0.1000\n",
      "Epoch 12/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.9356 - accuracy: 0.6341 - lr: 0.1000\n",
      "Epoch 13/26\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.8587 - accuracy: 0.6654 - lr: 0.1000\n",
      "Epoch 14/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.8026 - accuracy: 0.6938 - lr: 0.1000\n",
      "Epoch 15/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.7454 - accuracy: 0.6967 - lr: 0.1000\n",
      "Epoch 16/26\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.7238 - accuracy: 0.7251 - lr: 0.1000\n",
      "Epoch 17/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.6544 - accuracy: 0.7678 - lr: 0.1000\n",
      "Epoch 18/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.5785 - accuracy: 0.7934 - lr: 0.1000\n",
      "Epoch 19/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.5284 - accuracy: 0.7972 - lr: 0.1000\n",
      "Epoch 20/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.4714 - accuracy: 0.8322 - lr: 0.1000\n",
      "Epoch 21/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.5582 - accuracy: 0.7905 - lr: 0.1000\n",
      "Epoch 22/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.6461 - accuracy: 0.7583 - lr: 0.1000\n",
      "Epoch 23/26\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5152 - accuracy: 0.8018\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.5164 - accuracy: 0.8009 - lr: 0.1000\n",
      "Epoch 24/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.6973 - accuracy: 0.7431 - lr: 0.0100\n",
      "Epoch 25/26\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.4893 - accuracy: 0.8171 - lr: 0.0100\n",
      "Epoch 26/26\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.3694 - accuracy: 0.8616 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "history = model_mfe.fit(\n",
    "    train_mfe_feat,    \n",
    "    y_train,\n",
    "    epochs = 26,\n",
    "    batch_size = 64,\n",
    "    callbacks = [lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c5fa88",
   "metadata": {},
   "source": [
    "### MFCC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9766c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 10:16:03.863496: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-12-17 10:16:03.863569: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: dh-node19\n",
      "2024-12-17 10:16:03.863586: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: dh-node19\n",
      "2024-12-17 10:16:03.863704: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got \"1\"\n",
      "2024-12-17 10:16:03.863746: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 555.42.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 13, 50, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 13, 50, 32)           288       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 13, 50, 32)           128       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 13, 50, 32)           0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d (Average  (None, 3, 16, 32)            0         ['re_lu[0][0]']               \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 3, 16, 32)            9216      ['average_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 3, 16, 32)            0         ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 3, 16, 32)            128       ['re_lu_1[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 3, 16, 32)            9216      ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 3, 16, 32)            0         ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 3, 16, 32)            128       ['re_lu_2[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 3, 16, 32)            0         ['average_pooling2d[0][0]',   \n",
      "                                                                     'batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 3, 16, 32)            9216      ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 3, 16, 32)            0         ['conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 3, 16, 32)            128       ['re_lu_3[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 3, 16, 32)            9216      ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 3, 16, 32)            0         ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 3, 16, 32)            128       ['re_lu_4[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 3, 16, 32)            0         ['add[0][0]',                 \n",
      "                                                                     'batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 3, 16, 32)            9216      ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 3, 16, 32)            0         ['conv2d_5[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 3, 16, 32)            128       ['re_lu_5[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 3, 16, 32)            9216      ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)              (None, 3, 16, 32)            0         ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 3, 16, 32)            128       ['re_lu_6[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 3, 16, 32)            0         ['add_1[0][0]',               \n",
      "                                                                     'batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling2d (  (None, 32)                   0         ['add_2[0][0]']               \n",
      " GlobalAveragePooling2D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 6)                    198       ['global_average_pooling2d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 56678 (221.40 KB)\n",
      "Trainable params: 56230 (219.65 KB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.1, \n",
    "    momentum=0.9, \n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# learning rate scheduler\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "input_shape_mfcc = (mfcc_height, mfcc_width, 1) # (height, width, channels)\n",
    "\n",
    "# build the model\n",
    "model_mfcc = build_resnet_8(input_shape_mfcc, num_classes=6, feat_maps=32) # red, green, blue, one, two, three\n",
    "\n",
    "# model summary\n",
    "model_mfcc.summary()\n",
    "\n",
    "# compile the model\n",
    "model_mfcc.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5d54682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/26\n",
      "17/17 [==============================] - 2s 13ms/step - loss: 1.4853 - accuracy: 0.3867 - lr: 0.1000\n",
      "Epoch 2/26\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 1.0902 - accuracy: 0.5649 - lr: 0.1000\n",
      "Epoch 3/26\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.9252 - accuracy: 0.6332 - lr: 0.1000\n",
      "Epoch 4/26\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7303 - accuracy: 0.7261 - lr: 0.1000\n",
      "Epoch 5/26\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6184 - accuracy: 0.7649 - lr: 0.1000\n",
      "Epoch 6/26\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5644 - accuracy: 0.7924 - lr: 0.1000\n",
      "Epoch 7/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4787 - accuracy: 0.8427 - lr: 0.1000\n",
      "Epoch 8/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4582 - accuracy: 0.8351 - lr: 0.1000\n",
      "Epoch 9/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3981 - accuracy: 0.8588 - lr: 0.1000\n",
      "Epoch 10/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3294 - accuracy: 0.8825 - lr: 0.1000\n",
      "Epoch 11/26\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2761 - accuracy: 0.8976 - lr: 0.1000\n",
      "Epoch 12/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2342 - accuracy: 0.9194 - lr: 0.1000\n",
      "Epoch 13/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.1842 - accuracy: 0.9336 - lr: 0.1000\n",
      "Epoch 14/26\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2202 - accuracy: 0.9270 - lr: 0.1000\n",
      "Epoch 15/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.1547 - accuracy: 0.9479 - lr: 0.1000\n",
      "Epoch 16/26\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.1117 - accuracy: 0.9602 - lr: 0.1000\n",
      "Epoch 17/26\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0842 - accuracy: 0.9754 - lr: 0.1000\n",
      "Epoch 18/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0853 - accuracy: 0.9773 - lr: 0.1000\n",
      "Epoch 19/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.1586 - accuracy: 0.9450 - lr: 0.1000\n",
      "Epoch 20/26\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2746 - accuracy: 0.9072\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2741 - accuracy: 0.9071 - lr: 0.1000\n",
      "Epoch 21/26\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4081 - accuracy: 0.8626 - lr: 0.0100\n",
      "Epoch 22/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.1838 - accuracy: 0.9336 - lr: 0.0100\n",
      "Epoch 23/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0743 - accuracy: 0.9858 - lr: 0.0100\n",
      "Epoch 24/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0607 - accuracy: 0.9858 - lr: 0.0100\n",
      "Epoch 25/26\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0394 - accuracy: 0.9972 - lr: 0.0100\n",
      "Epoch 26/26\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0254 - accuracy: 0.9991 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "history = model_mfcc.fit(\n",
    "    train_mfcc_feat,    \n",
    "    y_train,\n",
    "    epochs = 26,\n",
    "    batch_size = 64,\n",
    "    callbacks = [lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92adf458",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "ResNet outputs probabilities of how likely a a sample is of the 6 classes. The final layer is a dense (or fully connected) layer with a softmax activation. \n",
    "To get the label and compare to true labels will take the argmax (whichever class of each sample had the highest probabilty). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f146564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFE\n",
    "y_pred_mfe_probs = model_mfe.predict(test_mfe_feat, batch_size=64)\n",
    "y_pred_mfe = np.argmax(y_pred_mfe_probs, axis=1)\n",
    "\n",
    "accur_mfe = accuracy_score(y_test, y_pred_mfe)\n",
    "weighted_accur_mfe = balanced_accuracy_score(y_test, y_pred_mfe)\n",
    "print(f'MFE Feature Accuracy: {accur_mfe * 100:.2f}%')\n",
    "print(f'MFE Feature Weighted Accuracy: {weighted_accur_mfe * 100:.2f}%')\n",
    "\n",
    "\n",
    "# MFCC\n",
    "y_pred_mfcc_probs = model_mfcc.predict(test_mfcc_feat, batch_size=64)\n",
    "y_pred_mfcc = np.argmax(y_pred_mfcc_probs, axis=1)\n",
    "\n",
    "accur_mfcc = accuracy_score(y_test, y_pred_mfcc)\n",
    "weighted_accur_mfcc = balanced_accuracy_score(y_test, y_pred_mfcc)\n",
    "print(f'MFCC Feature Accuracy: {accur_mfcc * 100:.2f}%')\n",
    "print(f'MFCC Feature Weighted Accuracy: {weighted_accur_mfcc * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb247c",
   "metadata": {},
   "source": [
    "## Quantization & Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2bc4ea",
   "metadata": {},
   "source": [
    "#### MFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34286dbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# converting to TensorFlow Lite with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_mfe)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # enabling default optimizations\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# saving model\n",
    "model_path_mfe = f'{save_model_path}/resnet8_mfe_nmel_{n_mel}_rand_{random_state}_32_feat.tflite'\n",
    "with open(model_path_mfe, \"wb\") as f:\n",
    "    f.write(tflite_model)  # `tflite_model` is the quantized model from earlier steps\n",
    "\n",
    "# obtain model size\n",
    "model_size_bytes = os.path.getsize(model_path_mfe)\n",
    "model_size_kb = model_size_bytes / 1024  # Convert bytes to KB\n",
    "print(f\"Model size: {model_size_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bd59c",
   "metadata": {},
   "source": [
    "#### MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38829322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 68.05 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 10:16:22.576042: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-12-17 10:16:22.576079: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-12-17 10:16:22.576413: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmps68cho1r\n",
      "2024-12-17 10:16:22.579662: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-12-17 10:16:22.579683: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmps68cho1r\n",
      "2024-12-17 10:16:22.585722: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-12-17 10:16:22.588305: I tensorflow/cc/saved_model/loader.cc:235] Restoring SavedModel bundle.\n",
      "2024-12-17 10:16:22.687303: I tensorflow/cc/saved_model/loader.cc:219] Running initialization op on SavedModel bundle at path: /tmp/tmps68cho1r\n",
      "2024-12-17 10:16:22.714871: I tensorflow/cc/saved_model/loader.cc:336] SavedModel load for tags { serve }; Status: success: OK. Took 138459 microseconds.\n",
      "2024-12-17 10:16:22.784971: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 18, Total Ops 53, % non-converted = 33.96 %\n",
      " * 18 ARITH ops\n",
      "\n",
      "- arith.constant:   18 occurrences  (f32: 17, i32: 1)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 9)\n",
      "  (f32: 1)\n",
      "  (f32: 7)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (f32: 6)\n",
      "  (uq_8: 6)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "# converting to TensorFlow Lite with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_mfcc)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # enabling default optimizations\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# saving model\n",
    "model_path_mfcc = f'{save_model_path}/resnet8_mfcc_nmel_{n_mel}_rand_{random_state}_32_feat.tflite'\n",
    "with open(model_path_mfcc, \"wb\") as f:\n",
    "    f.write(tflite_model)  # `tflite_model` is the quantized model from earlier steps\n",
    "\n",
    "# obtain model size\n",
    "model_size_bytes = os.path.getsize(model_path_mfcc)\n",
    "model_size_kb = model_size_bytes / 1024  # Convert bytes to KB\n",
    "print(f\"Model size: {model_size_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f63111b",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "\n",
    "See how much accuracy has changed after quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fafe06",
   "metadata": {},
   "source": [
    "### Final MFE Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuarcy_after_quan(model_path_mfe, test_mfe_feat, y_test, 'mfe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34d3d6",
   "metadata": {},
   "source": [
    "### Final MFCC Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519344eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuarcy_after_quan(model_path_mfcc, test_mfcc_feat, y_test, 'mfcc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f212f4",
   "metadata": {},
   "source": [
    "### MFE ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_multiclass_metrics(y_test, y_pred_mfe_probs, label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b01612a",
   "metadata": {},
   "source": [
    "### MFCC ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90753f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_multiclass_metrics(y_test, y_pred_mfcc_probs, label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b214cc",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2293cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_mfcc)\n",
    "\n",
    "# displaying confusion matrix as a heatmap\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=label_mapping.values())\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Random Forest: Confusion Matrix\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f55e860",
   "metadata": {},
   "source": [
    "## Visualize Model Archiecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88923d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=1393x300 at 0x7FAA0A784DF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def text_callable(layer_index, layer):\n",
    "    # Every other piece of text is drawn above the layer, the first one below\n",
    "    above = bool(layer_index%2)\n",
    "\n",
    "    # Get the output shape of the layer\n",
    "    output_shape = [x for x in list(layer.output_shape) if x is not None]\n",
    "\n",
    "    # If the output shape is a list of tuples, we only take the first one\n",
    "    if isinstance(output_shape[0], tuple):\n",
    "        output_shape = list(output_shape[0])\n",
    "        output_shape = [x for x in output_shape if x is not None]\n",
    "\n",
    "    # Variable to store text which will be drawn    \n",
    "    output_shape_txt = \"\"\n",
    "\n",
    "    # Create a string representation of the output shape\n",
    "    for ii in range(len(output_shape)):\n",
    "        output_shape_txt += str(output_shape[ii])\n",
    "        if ii < len(output_shape) - 2: # Add an x between dimensions, e.g. 3x3\n",
    "            output_shape_txt += \"x\"\n",
    "        if ii == len(output_shape) - 2: # Add a newline between the last two dimensions, e.g. 3x3 \\n 64\n",
    "            output_shape_txt += \"\\n\"\n",
    "\n",
    "    # Add the name of the layer to the text, as a new line\n",
    "    output_shape_txt += f\"\\n{layer.name}\"\n",
    "\n",
    "    # Return the text value and if it should be drawn above the layer\n",
    "    return output_shape_txt, above\n",
    "\n",
    "visualkeras.layered_view(\n",
    "    model_mfcc,\n",
    "    text_callable=text_callable, spacing=30,\n",
    "    padding=5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf71820",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8d695",
   "metadata": {},
   "source": [
    "[1] Tang, Raphael, and Jimmy Lin. Deep residual learning for small-footprint keyword spotting. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 21 Sept. 2018, https://doi.org/10.1109/icassp.2018.8462688. \n",
    "\n",
    "[2] Jongboom, Jan, et al. Edgeimpulse/Processing-Blocks: Signal Processing Blocks. GitHub, Edge Impulse, 3 Mar. 2022, github.com/edgeimpulse/processing-blocks. \n",
    "\n",
    "[3] https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#one-vs-rest-multiclass-roc\n",
    "\n",
    "[4] https://github.com/paulgavrikov/visualkeras/tree/master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
